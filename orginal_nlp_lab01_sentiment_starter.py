# -*- coding: utf-8 -*-
"""orginal NLP_Lab01_sentiment_starter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1861uzQ22HDnhOlkxvsd1pYilLF-Y9qx0

# Lab 01: Sentiment analysis

Team members:
- The Lazy one : Ghodbane Youcef Islam
- The Slave : Boucenna Abderrahmane
- The Know-it-all one : Adjlane Aymen Abdeldjalil
- The What-is-happening one : Attou yassine
"""

import nltk
import numpy as np
import scipy
import tensorflow as tf
from tensorflow import keras

"""## 1. Data preparation


### 1.1. Data lecture
"""

import pandas as pd
data = pd.read_csv('finance_sentiment.csv')

data.head(10)

from sklearn.model_selection import train_test_split

Xtxt_train, Xtxt_test, Y_train, Y_test = train_test_split(np.array(data["Sentence"]), np.array(data["Sentiment"]), test_size=0.3, random_state=0)
Xtxt_train.shape

from sklearn.preprocessing import LabelBinarizer

label_binarizer = LabelBinarizer()
output = label_binarizer.fit_transform(Y_train)

output

"""### 1.2. Words' encoding"""

# Our basic tokenizer using regular expressions
import re

token_pattern = re.compile(r'(?u)\b\w\w+\b')
tokenizer = token_pattern.findall


# Example
tokens = tokenizer('The students are trying to understand this code')
tokens

tokenized_sentences = []
for sentence in Xtxt_train:
    tokenized_sentences.append(tokenizer(sentence))

print(tokenized_sentences)

from gensim.models import Word2Vec
# TODO: train a Word2Vec model
# ==========================================
model_w2v = Word2Vec(tokenized_sentences, vector_size=15, window=3, min_count=1)

model_w2v

from gensim.models import FastText
# TODO: train a Fasttext model
# ==========================================
fasttext_model = FastText(tokenized_sentences, vector_size=15, window=5, min_count=1)

fasttext_model

"""### 1.3. Supporting functions"""

from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
# TODO implement tokenstem
# =============================================
nltk.download('stopwords')

def tokenstem(sentence):
    porter_stemmer = PorterStemmer()
    tokenized_sentence = []
    for token in tokenizer(sentence):
        if not token in nltk.corpus.stopwords.words("english"):
            tokenized_sentence.append(token)
    return [porter_stemmer.stem(word) for word in tokenized_sentence]

def sentvectEncode(wv, sentWords, mx=20):
    centroids = []
    concats = []
    matrices = []

    for sent in sentWords:
        # Initialize vectors for each sentence
        sent_vectors = []

        for word in sent:
            try:
                # Try to encode the word using wv
                word_vect = wv[word]
            except KeyError:
                # If the word is not in the vocabulary, use a zero vector
                word_vect = np.zeros(15)

            sent_vectors.append(word_vect)

        # Centroid encoding
        centroid = np.mean(sent_vectors, axis=0)
        centroids.append(centroid)

        # Concat encoding
        concat = np.concatenate(sent_vectors)[:mx * 15]
        if len(concat) < mx * 15:
            concat = np.pad(concat, (0, mx * 15 - len(concat)), 'constant')
        concats.append(concat)

        # Matrix encoding
        matrix = np.array(sent_vectors[:mx])
        if matrix.shape[0] < mx:
            padding = np.zeros((mx - matrix.shape[0], 15))
            matrix = np.vstack([matrix, padding])
        matrices.append(matrix)

    return np.array(centroids), np.array(concats), np.array(matrices)

"""### 1.4. Sentence representation"""

from sklearn.feature_extraction.text import CountVectorizer
# TODO: TF vectorization
# =============================================
vectorizer = CountVectorizer(tokenizer=tokenstem,max_features=3000)
X_train = vectorizer.fit_transform(Xtxt_train)
X_test = vectorizer.fit_transform(Xtxt_test)

# TODO: Word2Vec sentences' encodings
# ====================================
w2v_centroids_train, w2v_concat_train, w2v_matrix_train = sentvectEncode(model_w2v.wv, [tokenstem(x) for x in Xtxt_train])
w2v_centroids_test, w2v_concat_test, w2v_matrix_test = sentvectEncode(model_w2v.wv, [tokenstem(x) for x in Xtxt_test])

# TODO: Fasttext sentences' encodings
# ====================================
fasttext_centroids, fasttext_concat, fasttext_matrix = sentvectEncode(fasttext_model.wv, [tokenstem(x) for x in Xtxt_train])
fasttext_centroids_test, fasttext_concat_test, fasttext_matrix_test = sentvectEncode(fasttext_model.wv, [tokenstem(x) for x in Xtxt_test])

"""## 2. Training models

### 2.1. TF-based/MultinomialNB
"""

from sklearn.naive_bayes import MultinomialNB
# ==========================================


# Instantiate and train the model
TF_MNB = MultinomialNB()
TF_MNB.fit(X_train, Y_train)  # Assuming Y_train is not one-hot encode

"""### 2.2. Victor-based/MLP"""

from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
# TODO: design a function which creates a model with
# an input followed by a dense of 5 neurones/relu followed by an output layer
# =====================================================================

def create_mlp_model(input_dim, output_dim):
    input_layer = Input(shape=(input_dim,))
    dense_layer = Dense(5, activation='relu')(input_layer)
    output_layer = Dense(output_dim, activation='softmax')(dense_layer)
    model = Model(inputs=input_layer, outputs=output_layer)
    return model

# Instantiate models for each encoding
output_dim = 3  # Number of unique classes

from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
def create_mlp_model2(input_dim, output_dim):
    input_layer = Input(shape=(input_dim,))

    # Add more dense layers
    dense_layer1 = Dense(128, activation='relu')(input_layer)  # First dense layer with 128 neurons
    dense_layer2 = Dense(64, activation='relu')(dense_layer1)  # Second dense layer with 64 neurons
    dense_layer3 = Dense(32, activation='relu')(dense_layer2)  # Third dense layer with 32 neurons

    # Output layer
    output_layer = Dense(output_dim, activation='softmax')(dense_layer3)

    # Create the model
    model = Model(inputs=input_layer, outputs=output_layer)
    return model

# Instantiate the model for each encoding
output_dim = 3  # Adjust this based on the number of unique classes in your dataset
input_dim = 100  # Adjust this based on the input dimension of your data

mlp_model = create_mlp_model(input_dim, output_dim)

# Model Summary
mlp_model.summary()

# W2VCent_MLP word2vec centroid with MLP
# =============================================
W2VCent_MLP = create_mlp_model2(15, output_dim)

W2VCent_MLP.summary()

# TODO: W2VCent_MLP word2vec centroid with MLP training
# =============================================
W2VCent_MLP.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])
W2VCent_MLP.fit(w2v_centroids_train, output, epochs=100)

# TODO: W2VConcat_MLP word2vec concatenation with MLP
# =============================================
W2VConcat_MLP = create_mlp_model(300, output_dim)
W2VConcat_MLP.summary()

# TODO: W2VConcat_MLP word2vec centroid with MLP training
# =================================================
W2VConcat_MLP.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])
W2VConcat_MLP.fit(w2v_concat_train, output, epochs=100)

# TODO: W2VCent_MLP fasttext centroid with MLP
# =============================================
FTCent_MLP = create_mlp_model(5, output_dim)
FTCent_MLP.summary()

# TODO: W2VCent_MLP fasttext centroid with MLP training
# =============================================
FTCent_MLP.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])
FTCent_MLP.fit(fasttext_centroids, output, epochs=100)

# TODO: FTConcat_MLP fasttext concatenation with MLP
# =============================================
FTConcat_MLP = create_mlp_model(100, output_dim)
FTConcat_MLP.summary()

# TODO: FTConcat_MLP fasttext concatenation with MLP training
# =============================================
FTConcat_MLP.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])
FTConcat_MLP.fit(fasttext_concat, output, epochs=100)

"""### 2.3. Matrix-based/CNN"""

from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten
from tensorflow.keras.models import Model
# TODO: design a function which creates a model with
# an input followed by a conv1d (5 filters, 3 window) followed by a maxpool (strides=1)
# followed by a flatten, then a dense (output)
# =====================================================================
def create_cnn_model(input_shape, output_dim):
    # Define the input layer
    input_layer = Input(shape=input_shape)

    # Add a Conv1D layer
    conv_layer = Conv1D(filters=5, kernel_size=3, activation='relu')(input_layer)

    # Add a MaxPool1D layer
    pool_layer = MaxPool1D(strides=1)(conv_layer)

    # Add a Flatten layer
    flatten_layer = Flatten()(pool_layer)

    # Add a Dense (output) layer
    output_layer = Dense(output_dim, activation='softmax')(flatten_layer)  # Use 'sigmoid' for binary classification

    # Create the model
    model = Model(inputs=input_layer, outputs=output_layer)

    return model

# Example Usage
output_dim = 3  # Replace with the number of classes in your dataset
input_shape = (20, 5)  # Replace with the appropriate input shape

# TODO: W2VMat_CNN word2vec matrix with CNN
# =============================================
W2VMat_CNN = create_cnn_model(input_shape, output_dim)
W2VMat_CNN.summary()

# TODO: W2VMat_CNN word2vec matrix with CNN training
# =============================================
W2VMat_CNN.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])
W2VMat_CNN.fit(w2v_matrix_train, output, epochs=100)

# TODO: WFTMat_CNN fasttext matrix with CNN
# =============================================
FTMat_CNN = create_cnn_model(input_shape, output_dim)
FTMat_CNN.summary()

# TODO: WFTMat_CNN fasttext matrix with CNN training
# =============================================
FTMat_CNN.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])
FTMat_CNN.fit(fasttext_matrix, output, epochs=100)

"""## 3. Testing models"""

import timeit
from sklearn.metrics import classification_report

# TODO: complete
# ==============
t = timeit.default_timer()
Ypred = TF_MNB.predict(X_test)
print('m1', timeit.default_timer()-t)

print(classification_report(Y_test, Ypred, zero_division=0))

# TODO: complete
# ==============
t = timeit.default_timer()
Ypred = W2VCent_MLP.predict(w2v_centroids_test)
print('W2VCent_MLP', timeit.default_timer()-t)

print(classification_report(Y_test, label_binarizer.inverse_transform(Ypred), zero_division=0))

# TODO: complete
# ==============
t = timeit.default_timer()
Ypred = W2VConcat_MLP.predict(w2v_concat_test)
print('W2VConcat_MLP', timeit.default_timer()-t)

print(classification_report(Y_test, label_binarizer.inverse_transform(Ypred), zero_division=0))

# TODO: complete
# ==============
t = timeit.default_timer()
Ypred = FTCent_MLP.predict(fasttext_centroids_test)
print('FTCent_MLP', timeit.default_timer()-t)

print(classification_report(Y_test, label_binarizer.inverse_transform(Ypred), zero_division=0))

# TODO: complete
# ==============
t = timeit.default_timer()
Ypred = FTConcat_MLP.predict(fasttext_concat_test)
print('FTConcat_MLP', timeit.default_timer()-t)

print(classification_report(Y_test, label_binarizer.inverse_transform(Ypred), zero_division=0))

# TODO: complete
# ==============
t = timeit.default_timer()
Ypred = W2VMat_CNN.predict(w2v_matrix_test)
print('FTConcat_MLP', timeit.default_timer()-t)

print(classification_report(Y_test, label_binarizer.inverse_transform(Ypred), zero_division=0))

# TODO: complete
# ==============
t = timeit.default_timer()
Ypred = FTMat_CNN.predict(fasttext_matrix_test)
print('FTConcat_MLP', timeit.default_timer()-t)

print(classification_report(Y_test, label_binarizer.inverse_transform(Ypred), zero_division=0))