# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d8OIzDs9MJgLVAoufRohWF6GhRaOyFfV
"""

import numpy as np
import pandas as pd
import tensorflow as tf

data = pd.read_csv('finance_sentiment.csv')

data.head(10)

from sklearn.model_selection import train_test_split

Xtxt_train, Xtxt_test, Y_train, Y_test = train_test_split(np.array(data["Sentence"]), np.array(data["Sentiment"]), test_size=0.3, random_state=0)
Xtxt_train.shape, Xtxt_test.shape

from transformers import BertTokenizer, TFBertForSequenceClassification

# Load tokenizer and BERT model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)

from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

# Prepare the training data
train_texts = list(Xtxt_train)
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(Y_train)
Y_train_one_hot = to_categorical(integer_encoded, num_classes=3)  # 3 classes: neutral, positive, negative

# Tokenize training data
train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors="tf", max_length=512)

# Training the model
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)
loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

bert_model.fit(
    {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},
    Y_train_one_hot,
    epochs=3,
    batch_size=8
)

# One-hot encode the test labels
test_integer_encoded = label_encoder.transform(Y_test)
Y_test_one_hot = to_categorical(test_integer_encoded, num_classes=3)

# Tokenize test data
test_encodings = tokenizer(list(Xtxt_test), padding=True, truncation=True, return_tensors="tf", max_length=512)

# Evaluate the model
evaluation = bert_model.evaluate(
    {'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']},
    Y_test_one_hot
)


print("Test Loss:", evaluation[0])
print("Test Accuracy:", evaluation[1])

from sklearn.metrics import classification_report

# Predicting on test data
predictions = bert_model.predict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']})
predicted_classes = tf.argmax(predictions.logits, axis=1).numpy()

# Convert one-hot encoded test labels back to label encoding for comparison
true_classes = tf.argmax(Y_test_one_hot, axis=1).numpy()

# Generate classification report
report = classification_report(true_classes, predicted_classes, target_names=label_encoder.classes_)
print(report)

"""**LSTM Model **"""

# Make a directory named kaggle and copy the kaggle.json file there.
!mkdir -p ~/.kaggle
!cp /content/kaggle.json ~/.kaggle/

# Change the permission of the file
!chmod 600 ~/.kaggle/kaggle.json

# Download the dataset
!kaggle datasets download -d umbertogriffo/googles-trained-word2vec-model-in-python
# Extract the data
!mkdir /content/googles-trained-word2vec-model
!unzip /content/googles-trained-word2vec-model-in-python.zip -d /content/googles-trained-word2vec-model

from gensim.models import KeyedVectors

word2vec_pretrained = KeyedVectors.load_word2vec_format("/content/googles-trained-word2vec-model/GoogleNews-vectors-negative300.bin",binary=True)
word2vec_pretrained_dict = dict(zip(word2vec_pretrained.key_to_index.keys(), word2vec_pretrained.vectors))

from keras.preprocessing.sequence import pad_sequences

token = tf.keras.preprocessing.text.Tokenizer(num_words=None)

token.fit_on_texts(list(Xtxt_train) + list(Xtxt_test))

xtrain_seq = token.texts_to_sequences(Xtxt_train) # text to sequences converts the sentence words to number sequences
xtest_seq = token.texts_to_sequences(Xtxt_test)

#zero pad sequences
xtrain_pad = pad_sequences(xtrain_seq,padding='post') # zero padding all sentences to have the same shape as the largest one
xtest_pad = pad_sequences(xtest_seq,padding='post')


word_index = token.word_index

embedding_matrix = np.zeros((len(word_index)+1, 300))
for word,i in word_index.items():
    embedding_vector = word2vec_pretrained_dict.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

# Build Custom Metrics (F1-Score)
from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m_():
    def f1_m(y_true, y_pred):
        precision = precision_m(y_true, y_pred)
        recall = recall_m(y_true, y_pred)
        return 2*((precision*recall)/(precision+recall+K.epsilon()))
    return f1_m

from keras.layers import Embedding, Dropout, Activation, GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D

model = Sequential()
model.add(Embedding(len(word_index)+1,300,weights=[embedding_matrix], trainable = False))

model.add(SpatialDropout1D(0.3))
model.add(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3))

model.add(Dense(1024 , activation = 'relu'))
model.add(Dropout(0.8))

model.add(Dense(1024, activation = 'relu'))
model.add(Dropout(0.8))

model.add(Dense(3))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = [f1_m_(), 'acc'])

model.fit(xtrain_pad, y=output, batch_size=512, epochs=100, verbose=1)

import timeit
from sklearn.metrics import classification_report

t = timeit.default_timer()
Ypred = model.predict(xtest_pad)
print('LSTM', timeit.default_timer()-t)

print(classification_report(Y_test, label_binarizer.inverse_transform(y), zero_division=0))